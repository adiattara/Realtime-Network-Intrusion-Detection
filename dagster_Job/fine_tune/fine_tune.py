import os
import json
import pickle
import psycopg2
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import load_model
from sklearn.metrics import classification_report, accuracy_score
from model import construire_ann  # ta fonction pour cr√©er le mod√®le
import mlflow
import mlflow.keras
import mlflow.sklearn
from datetime import datetime

# === CONFIGURATION ===
import os

# Configuration MLflow
os.environ["MLFLOW_TRACKING_URI"] = os.environ.get("MLFLOW_TRACKING_URI", "http://mlflow:5000")
EXPERIMENT_NAME = "network_anomaly_detection"
REGISTERED_MODEL_NAME = "network_anomaly_detection_model"

# Explication du Model Registry:
# Le Model Registry de MLflow permet de g√©rer le cycle de vie des mod√®les avec:
# - Versionnement automatique: chaque mod√®le enregistr√© re√ßoit un num√©ro de version
# - Gestion des √©tapes (stages): Development, Staging, Production, Archived
# - Transitions entre √©tapes: un mod√®le peut passer de Staging √† Production
# - Tra√ßabilit√©: chaque mod√®le est li√© √† l'exp√©rience qui l'a produit
#
# Dans ce script:
# - Les nouveaux mod√®les performants sont enregistr√©s et mis en "Production"
# - Les anciens mod√®les de production sont archiv√©s
# - Les mod√®les moins performants sont enregistr√©s en "Staging" pour √©valuation

DB_CONFIG = {
    "host": os.environ.get("DB_HOST", "network_db"),
    "port": int(os.environ.get("DB_PORT", 5432)),
    "database": os.environ.get("DB_NAME", "networkdb"),
    "user": os.environ.get("DB_USER", "user"),
    "password": os.environ.get("DB_PASSWORD", "password")
}
MODEL_PATH = "model.h5"
SCALER_PATH = "scaler.pkl"
X_BLIND_PATH = "X_blind_test.pkl"
Y_BLIND_PATH = "y_blind_test.pkl"

FEATURES = [
    "total_bytes", "pkt_count", "psh_count", "fwd_bytes", "bwd_bytes",
    "fwd_pkts", "bwd_pkts", "dport", "duration_ms", "flow_pkts_per_s", "fwd_bwd_ratio"
]

# === CHARGER LES DONN√âES DE FEEDBACK ===
def load_feedback_flows():
    conn = psycopg2.connect(**DB_CONFIG)
    df = pd.read_sql("SELECT flow_data, label_humain FROM reported_flows", conn)
    print("Colonnes pr√©sentes dans la table reported_flows:")
    print(df.columns.tolist())
    conn.close()

    # Parsing JSON
    def extract_features(flow_json):
        try:
            flow = json.loads(flow_json)
            return [flow.get(f, 0) for f in FEATURES]
        except Exception as e:
            print(f"Erreur parsing JSON: {e}")
            return [0] * len(FEATURES)

    features_df = df["flow_data"].apply(extract_features).apply(pd.Series)
    features_df.columns = FEATURES

    # Convertir le label texte en entier
    features_df["label"] = df["label_humain"].map({"Normal": 0, "Mal": 1})

    return features_df.dropna()

# === √âVALUATION ===
def evaluate_model(model, X, y, label=""):
    y_pred = (model.predict(X) > 0.5).astype(int)
    print(f"\nüìä Rapport de classification : {label}")
    report = classification_report(y, y_pred, output_dict=True)
    print(classification_report(y, y_pred))
    accuracy = accuracy_score(y, y_pred)

    # Retourner les m√©triques pour MLflow
    metrics = {
        "accuracy": accuracy,
        "precision_0": report["0"]["precision"],
        "recall_0": report["0"]["recall"],
        "f1_score_0": report["0"]["f1-score"],
        "precision_1": report["1"]["precision"],
        "recall_1": report["1"]["recall"],
        "f1_score_1": report["1"]["f1-score"],
    }

    return accuracy, metrics

# === PIPELINE PRINCIPAL ===
def main():
    # Configurer MLflow
    print("\n=== Configuration MLflow ===")
    print(f"MLFLOW_TRACKING_URI: {os.environ.get('MLFLOW_TRACKING_URI', 'Non d√©fini')}")
    print(f"EXPERIMENT_NAME: {EXPERIMENT_NAME}")
    print(f"REGISTERED_MODEL_NAME: {REGISTERED_MODEL_NAME}")

    # V√©rifier que MLflow est accessible
    try:
        # Tester la connexion au serveur MLflow
        import requests
        mlflow_uri = os.environ.get('MLFLOW_TRACKING_URI', 'http://mlflow:5000')
        print(f"üîç Test de connexion au serveur MLflow √† {mlflow_uri}...")

        try:
            response = requests.get(mlflow_uri)
            if response.status_code == 200:
                print(f"‚úÖ Connexion au serveur MLflow r√©ussie (status code: {response.status_code})")
            else:
                print(f"‚ö†Ô∏è Connexion au serveur MLflow √©tablie mais avec un status code inattendu: {response.status_code}")
        except Exception as conn_err:
            print(f"‚ùå Erreur de connexion au serveur MLflow: {conn_err}")
            print("‚ö†Ô∏è Cela peut indiquer un probl√®me de r√©seau ou que le serveur MLflow n'est pas accessible")

        # Configurer l'exp√©rience MLflow
        mlflow.set_experiment(EXPERIMENT_NAME)
        print(f"‚úÖ MLflow accessible, exp√©rience '{EXPERIMENT_NAME}' configur√©e")

        # Afficher les informations sur l'exp√©rience
        from mlflow.tracking import MlflowClient
        client = MlflowClient()
        experiment = client.get_experiment_by_name(EXPERIMENT_NAME)
        if experiment:
            print(f"‚ÑπÔ∏è Exp√©rience ID: {experiment.experiment_id}")
            print(f"‚ÑπÔ∏è Artefact Location: {experiment.artifact_location}")
        else:
            print(f"‚ö†Ô∏è L'exp√©rience '{EXPERIMENT_NAME}' n'a pas √©t√© trouv√©e")
    except Exception as e:
        print(f"‚ùå Erreur lors de la configuration de MLflow: {e}")
        print("‚ö†Ô∏è Tentative de continuer malgr√© l'erreur...")

    print("üì• Chargement des donn√©es signal√©es...")
    df = load_feedback_flows()

    X_new = df[FEATURES].values
    y_new = df["label"].astype(int).values

    print("üß™ Chargement des assets...")
    with open(SCALER_PATH, "rb") as f:
        scaler = pickle.load(f)
    with open(X_BLIND_PATH, "rb") as f:
        X_blind = pickle.load(f)
    with open(Y_BLIND_PATH, "rb") as f:
        y_blind = pickle.load(f)

    current_model = load_model(MODEL_PATH)

    # Appliquer le scaler
    if hasattr(scaler, 'transform'):
        X_scaled = scaler.transform(X_new)
    else:
        from sklearn.preprocessing import StandardScaler
        temp_scaler = StandardScaler()
        temp_scaler.fit(X_new)
        if isinstance(scaler, tuple) and len(scaler) == 2:
            temp_scaler.mean_ = scaler[0]
            temp_scaler.scale_ = scaler[1]
        X_scaled = temp_scaler.transform(X_new)

    # D√©marrer un run MLflow
    run_name = f"fine_tune_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    print(f"\n=== D√©marrage du run MLflow: {run_name} ===")

    with mlflow.start_run(run_name=run_name) as active_run:
        print(f"‚úÖ Run MLflow d√©marr√© avec succ√®s: {active_run.info.run_id}")
        # Afficher le chemin o√π les artefacts seront stock√©s
        artifact_uri = mlflow.get_artifact_uri()
        print(f"üìÅ Chemin des artefacts: {artifact_uri}")

        # V√©rifier si le chemin des artefacts est accessible
        try:
            # Cr√©er un fichier test pour v√©rifier l'acc√®s en √©criture
            test_file_path = os.path.join(os.getcwd(), "mlflow_test_file.txt")
            with open(test_file_path, "w") as f:
                f.write("Test MLflow artifact access")

            # Loguer le fichier test comme artefact
            mlflow.log_artifact(test_file_path, "test")
            print(f"‚úÖ Test d'acc√®s aux artefacts r√©ussi: fichier test logg√© √† {artifact_uri}/test/mlflow_test_file.txt")

            # Supprimer le fichier test local
            os.remove(test_file_path)
        except Exception as e:
            print(f"‚ùå Erreur lors du test d'acc√®s aux artefacts: {e}")
            print("‚ö†Ô∏è Cela peut indiquer un probl√®me d'acc√®s au stockage des artefacts")
        # Loguer les param√®tres
        mlflow.log_params({
            "epochs": 5,
            "batch_size": 32,
            "validation_split": 0.2,
            "input_features": len(FEATURES),
            "training_samples": len(X_new)
        })

        # Loguer le scaler
        mlflow.sklearn.log_model(scaler, "scaler")

        # √âvaluer l'ancien mod√®le
        print("üìä √âvaluation du mod√®le actuel...")
        acc_old, metrics_old = evaluate_model(current_model, X_blind, y_blind, label="Ancien mod√®le")

        # Loguer les m√©triques de l'ancien mod√®le
        for key, value in metrics_old.items():
            mlflow.log_metric(f"old_{key}", value)

        # Cr√©er nouveau mod√®le avec m√™mes poids
        print("üîÅ R√©entra√Ænement du mod√®le avec feedback...")
        new_model = construire_ann(X_scaled.shape[1])
        new_model.set_weights(current_model.get_weights())

        # Entra√Æner le mod√®le et capturer l'historique
        history = new_model.fit(
            X_scaled, y_new, 
            epochs=5, 
            batch_size=32, 
            validation_split=0.2,
            verbose=1
        )

        # Loguer les m√©triques d'entra√Ænement
        for epoch, (loss, acc, val_loss, val_acc) in enumerate(zip(
            history.history['loss'],
            history.history['accuracy'],
            history.history['val_loss'],
            history.history['val_accuracy']
        )):
            mlflow.log_metrics({
                "train_loss": loss,
                "train_accuracy": acc,
                "val_loss": val_loss,
                "val_accuracy": val_acc
            }, step=epoch)

        # √âvaluer le nouveau mod√®le
        acc_new, metrics_new = evaluate_model(new_model, X_blind, y_blind, label="Nouveau mod√®le")

        # Loguer les m√©triques du nouveau mod√®le
        for key, value in metrics_new.items():
            mlflow.log_metric(f"new_{key}", value)

        # Loguer l'am√©lioration
        improvement = acc_new - acc_old
        mlflow.log_metric("accuracy_improvement", improvement)

        # Comparaison et sauvegarde
        if acc_new >= acc_old:
            print("‚úÖ Nouveau mod√®le adopt√©, sauvegarde en cours...")
            new_model.save(MODEL_PATH)
            # Loguer le mod√®le dans MLflow
            print("üìù Logging du mod√®le dans MLflow...")
            try:
                # Sauvegarder le mod√®le localement d'abord
                model_abs_path = os.path.abspath(MODEL_PATH)
                print(f"üìù Sauvegarde du mod√®le localement √† {model_abs_path}...")
                new_model.save(MODEL_PATH)

                # V√©rifier que le fichier a bien √©t√© cr√©√©
                if os.path.exists(MODEL_PATH):
                    print(f"‚úÖ Mod√®le sauvegard√© localement √† {model_abs_path}")
                    print(f"   Taille du fichier: {os.path.getsize(MODEL_PATH)} octets")

                    # V√©rifier que le mod√®le peut √™tre recharg√©
                    try:
                        # Use the already imported load_model function
                        test_model = load_model(MODEL_PATH)
                        print(f"‚úÖ Mod√®le recharg√© avec succ√®s pour v√©rification")
                    except Exception as e:
                        print(f"‚ùå Erreur lors du rechargement du mod√®le pour v√©rification: {e}")
                else:
                    print(f"‚ùå √âchec de la sauvegarde du mod√®le: le fichier {model_abs_path} n'existe pas")

                # Loguer le mod√®le directement dans MLflow
                mlflow.keras.log_model(new_model, "model")
                print("‚úÖ Mod√®le logg√© dans MLflow avec mlflow.keras.log_model")

                # Loguer le fichier du mod√®le comme artefact
                print(f"üìù Logging du fichier mod√®le {MODEL_PATH} comme artefact...")
                mlflow.log_artifact(MODEL_PATH, "saved_model")

                # V√©rifier que l'artefact a bien √©t√© logg√©
                artifact_path = os.path.join("saved_model", os.path.basename(MODEL_PATH))
                try:
                    # Obtenir l'URI de l'artefact
                    artifact_uri = mlflow.get_artifact_uri(artifact_path)
                    print(f"‚úÖ Fichier mod√®le logg√© comme artefact √† {artifact_uri}")

                    # V√©rifier si l'URI est un chemin local ou distant
                    if artifact_uri.startswith("file:"):
                        local_path = artifact_uri.replace("file:", "")
                        if os.path.exists(local_path):
                            print(f"‚úÖ V√©rification: L'artefact existe √† {local_path}")
                            print(f"   Taille de l'artefact: {os.path.getsize(local_path)} octets")
                        else:
                            print(f"‚ùå V√©rification: L'artefact n'existe pas √† {local_path}")
                    else:
                        print(f"‚ÑπÔ∏è L'artefact est stock√© √† distance, impossible de v√©rifier directement")
                except Exception as e:
                    print(f"‚ùå Erreur lors de la v√©rification de l'artefact: {e}")

                # V√©rifier que le fichier existe localement
                if os.path.exists(MODEL_PATH):
                    print(f"‚úÖ V√©rification: Le fichier {MODEL_PATH} existe localement")
                else:
                    print(f"‚ùå V√©rification: Le fichier {MODEL_PATH} n'existe pas localement")

                mlflow.set_tag("model_status", "adopted")
            except Exception as e:
                print(f"‚ùå Erreur lors du logging du mod√®le: {e}")
                # Continuer malgr√© l'erreur pour ne pas bloquer le processus

            # Enregistrer le mod√®le dans le Model Registry
            try:
                model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
                registered_model = mlflow.register_model(
                    model_uri=model_uri,
                    name=REGISTERED_MODEL_NAME
                )
                print(f"‚úÖ Mod√®le enregistr√© dans le registry: {registered_model.name} version {registered_model.version}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'enregistrement du mod√®le dans le registry: {e}")
                print("‚ö†Ô∏è Le mod√®le a √©t√© sauvegard√© localement mais n'a pas pu √™tre enregistr√© dans le registry")
                return

            # Transition du mod√®le vers la production
            try:
                client = mlflow.tracking.MlflowClient()

                # V√©rifier s'il existe d√©j√† un mod√®le en production
                production_models = client.get_latest_versions(REGISTERED_MODEL_NAME, stages=["Production"])

                # D√©placer le nouveau mod√®le vers la production
                client.transition_model_version_stage(
                    name=REGISTERED_MODEL_NAME,
                    version=registered_model.version,
                    stage="Production"
                )
                print(f"‚úÖ Mod√®le version {registered_model.version} transitionn√© vers la Production")

                # Archiver les anciens mod√®les de production
                for model in production_models:
                    client.transition_model_version_stage(
                        name=REGISTERED_MODEL_NAME,
                        version=model.version,
                        stage="Archived"
                    )
                    print(f"üì¶ Ancien mod√®le version {model.version} archiv√©")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la transition du mod√®le: {e}")
                print("‚ö†Ô∏è Le mod√®le a √©t√© enregistr√© mais n'a pas pu √™tre transitionn√© vers la Production")
        else:
            print("‚ùå Nouveau mod√®le moins bon, conservation de l'ancien.")
            print("üìù Logging de l'ancien mod√®le dans MLflow...")
            try:
                # Loguer l'ancien mod√®le directement dans MLflow
                mlflow.keras.log_model(current_model, "model")
                print("‚úÖ Ancien mod√®le logg√© dans MLflow avec mlflow.keras.log_model")

                # Loguer le fichier du mod√®le comme artefact
                print(f"üìù Logging du fichier mod√®le {MODEL_PATH} comme artefact...")
                mlflow.log_artifact(MODEL_PATH, "saved_model")

                # V√©rifier que l'artefact a bien √©t√© logg√©
                artifact_path = os.path.join("saved_model", os.path.basename(MODEL_PATH))
                try:
                    # Obtenir l'URI de l'artefact
                    artifact_uri = mlflow.get_artifact_uri(artifact_path)
                    print(f"‚úÖ Fichier mod√®le logg√© comme artefact √† {artifact_uri}")

                    # V√©rifier si l'URI est un chemin local ou distant
                    if artifact_uri.startswith("file:"):
                        local_path = artifact_uri.replace("file:", "")
                        if os.path.exists(local_path):
                            print(f"‚úÖ V√©rification: L'artefact existe √† {local_path}")
                            print(f"   Taille de l'artefact: {os.path.getsize(local_path)} octets")
                        else:
                            print(f"‚ùå V√©rification: L'artefact n'existe pas √† {local_path}")
                    else:
                        print(f"‚ÑπÔ∏è L'artefact est stock√© √† distance, impossible de v√©rifier directement")
                except Exception as e:
                    print(f"‚ùå Erreur lors de la v√©rification de l'artefact: {e}")

                # V√©rifier que le fichier existe localement
                if os.path.exists(MODEL_PATH):
                    print(f"‚úÖ V√©rification: Le fichier {MODEL_PATH} existe localement")
                else:
                    print(f"‚ùå V√©rification: Le fichier {MODEL_PATH} n'existe pas localement")

                mlflow.set_tag("model_status", "rejected")
            except Exception as e:
                print(f"‚ùå Erreur lors du logging de l'ancien mod√®le: {e}")
                # Continuer malgr√© l'erreur pour ne pas bloquer le processus

            # Enregistrer quand m√™me l'ancien mod√®le dans le registry pour tra√ßabilit√©
            try:
                model_uri = f"runs:/{mlflow.active_run().info.run_id}/model"
                registered_model = mlflow.register_model(
                    model_uri=model_uri,
                    name=REGISTERED_MODEL_NAME
                )
                print(f"‚úÖ Ancien mod√®le enregistr√© dans le registry: {registered_model.name} version {registered_model.version}")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de l'enregistrement du mod√®le dans le registry: {e}")
                print("‚ö†Ô∏è Le mod√®le a √©t√© sauvegard√© localement mais n'a pas pu √™tre enregistr√© dans le registry")
                return

            # Transition du mod√®le vers le stage Staging pour √©valuation ult√©rieure
            try:
                client = mlflow.tracking.MlflowClient()
                client.transition_model_version_stage(
                    name=REGISTERED_MODEL_NAME,
                    version=registered_model.version,
                    stage="Staging"
                )
                print(f"üîç Mod√®le version {registered_model.version} transitionn√© vers Staging pour √©valuation ult√©rieure")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la transition du mod√®le vers Staging: {e}")
                print("‚ö†Ô∏è Le mod√®le a √©t√© enregistr√© mais n'a pas pu √™tre transitionn√© vers Staging")

if __name__ == "__main__":
    main()
